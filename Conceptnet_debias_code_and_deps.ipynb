{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with debias code from Conceptnet Numberbatch\n",
    "\n",
    "This notebook pulls all of the debiasing code and dependecies out of the Conceptnet repo. Spoilers: there's a lot of dependencies... skip to the bottom.\n",
    "\n",
    "\n",
    "- the repo with download links for embeddings https://github.com/commonsense/conceptnet-numberbatch\n",
    "- blog post discussing bias removal https://blog.conceptnet.io/2017/04/24/conceptnet-numberbatch-17-04-better-less-stereotyped-word-vectors/\n",
    "- actual debias code to play with here: https://github.com/commonsense/conceptnet5/blob/master/conceptnet5/vectors/debias.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also, then use that code to debias GloVe using the same method as Numberbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed code from file https://github.com/commonsense/conceptnet5/blob/master/conceptnet5/languages.py\n",
    "\n",
    "to satisfy: `from .languages import LCODE_ALIASES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LCODE_ALIASES = {\n",
    "    # Pretend that various Chinese languages and variants are equivalent. This\n",
    "    # is linguistically problematic, but it's also very helpful for aligning\n",
    "    # them on terms where they actually are the same.\n",
    "    #\n",
    "    # This would mostly be a problem if ConceptNet was being used to *generate*\n",
    "    # Chinese natural language text.\n",
    "    'cmn': 'zh',\n",
    "    'yue': 'zh',\n",
    "    'zh_tw': 'zh',\n",
    "    'zh_cn': 'zh',\n",
    "    'zh-tw': 'zh',\n",
    "    'zh-cn': 'zh',\n",
    "\n",
    "    'nds-de': 'nds',\n",
    "    'nds-nl': 'nds',\n",
    "\n",
    "    # An easier case: consider Bahasa Indonesia and Bahasa Malay to be the\n",
    "    # same language, with code 'ms', because they're already 90% the same.\n",
    "    # Many sources use 'ms' to represent the entire macrolanguage, with\n",
    "    # 'zsm' to refer to Bahasa Malay in particular.\n",
    "    'zsm': 'ms',\n",
    "    'id': 'ms',\n",
    "\n",
    "    # We had to make a decision here on Norwegian. Norwegian Bokmål ('nb') and\n",
    "    # Nynorsk ('nn') have somewhat different vocabularies but are mutually\n",
    "    # intelligible. Informal variants of Norwegian, especially when spoken,\n",
    "    # don't really distinguish them. Some Wiktionary entries don't distinguish\n",
    "    # them either. And the CLDR data puts them both in the same macrolanguage\n",
    "    # of Norwegian ('no').\n",
    "    #\n",
    "    # The catch is, Bokmål and Danish are *more* mutually intelligible than\n",
    "    # Bokmål and Nynorsk, so maybe they should be the same language too. But\n",
    "    # Nynorsk and Danish are less mutually intelligible.\n",
    "    #\n",
    "    # There is no language code that includes both Danish and Nynorsk, so\n",
    "    # it would probably be inappropriate to group them all together. We will\n",
    "    # take the easy route of making the language boundaries correspond to the\n",
    "    # national boundaries, and say that 'nn' and 'nb' are both kinds of 'no'.\n",
    "    #\n",
    "    # More information: http://languagelog.ldc.upenn.edu/nll/?p=9516\n",
    "    'nn': 'no',\n",
    "    'nb': 'no',\n",
    "\n",
    "    # Our sources have entries in Croatian, entries in Serbian, and entries\n",
    "    # in Serbo-Croatian. Some of the Serbian and Serbo-Croatian entries\n",
    "    # are written in Cyrillic letters, while all Croatian entries are written\n",
    "    # in Latin letters. Bosnian and Montenegrin are in there somewhere,\n",
    "    # too.\n",
    "    #\n",
    "    # Applying the same principle as Chinese, we will unify the language codes\n",
    "    # into the macrolanguage 'sh' without unifying the scripts.\n",
    "    'bs': 'sh',\n",
    "    'hr': 'sh',\n",
    "    'sr': 'sh',\n",
    "    'hbs': 'sh',\n",
    "    'sr-latn': 'sh',\n",
    "    'sr-cyrl': 'sh',\n",
    "\n",
    "    # More language codes that we would rather group into a broader language:\n",
    "    'arb': 'ar',   # Modern Standard Arabic -> Arabic\n",
    "    'arz': 'ar',   # Egyptian Arabic -> Arabic\n",
    "    'ary': 'ar',   # Moroccan Arabic -> Arabic\n",
    "    'ckb': 'ku',   # Central Kurdish -> Kurdish\n",
    "    'mvf': 'mn',   # Peripheral Mongolian -> Mongolian\n",
    "    'tl': 'fil',   # Tagalog -> Filipino\n",
    "    'vro': 'et',   # Võro -> Estonian\n",
    "    'sgs': 'lt',   # Samogitian -> Lithuanian\n",
    "    'ciw': 'oj',   # Chippewa -> Ojibwe\n",
    "    'xal': 'xwo',  # Kalmyk -> Oirat\n",
    "    'ffm': 'ff',   # Maasina Fulfulde -> Fula\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need the following from file https://github.com/commonsense/conceptnet5/blob/master/conceptnet5/language/english.py\n",
    "\n",
    "to satisfy: `from conceptnet5.language.english import english_filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS = [\n",
    "    'the', 'a', 'an'\n",
    "]\n",
    "\n",
    "DROP_FIRST = ['to']\n",
    "\n",
    "\n",
    "def english_filter(tokens):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, remove a small list of English stopwords.\n",
    "    \"\"\"\n",
    "    non_stopwords = [token for token in tokens if token not in STOPWORDS]\n",
    "    while non_stopwords and non_stopwords[0] in DROP_FIRST:\n",
    "        non_stopwords = non_stopwords[1:]\n",
    "    if non_stopwords:\n",
    "        return non_stopwords\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need the following from file https://github.com/commonsense/conceptnet5/blob/master/conceptnet5/uri.py\n",
    "\n",
    "to satisfy: `from conceptnet5.uri import concept_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_uri(*pieces):\n",
    "    \"\"\"\n",
    "    `join_uri` builds a URI from constituent pieces that should be joined\n",
    "    with slashes (/).\n",
    "    Leading and trailing on the pieces are acceptable, but will be ignored.\n",
    "    The resulting URI will always begin with a slash and have its pieces\n",
    "    separated by a single slash.\n",
    "    The pieces do not have `standardize_text` applied to them; to make sure your\n",
    "    URIs are in normal form, run `standardize_text` on each piece that represents\n",
    "    arbitrary text.\n",
    "    >>> join_uri('/c', 'en', 'cat')\n",
    "    '/c/en/cat'\n",
    "    >>> join_uri('c', 'en', ' spaces ')\n",
    "    '/c/en/ spaces '\n",
    "    >>> join_uri('/r/', 'AtLocation/')\n",
    "    '/r/AtLocation'\n",
    "    >>> join_uri('/test')\n",
    "    '/test'\n",
    "    >>> join_uri('test')\n",
    "    '/test'\n",
    "    >>> join_uri('/test', '/more/')\n",
    "    '/test/more'\n",
    "    \"\"\"\n",
    "    joined = '/' + ('/'.join([piece.strip('/') for piece in pieces]))\n",
    "    return joined\n",
    "\n",
    "\n",
    "def concept_uri(lang, text, *more):\n",
    "    \"\"\"\n",
    "    `concept_uri` builds a representation of a concept, which is a word or\n",
    "    phrase of a particular language, which can participate in relations with\n",
    "    other concepts, and may be linked to concepts in other languages.\n",
    "    Every concept has an ISO language code and a text. It may also have a part\n",
    "    of speech (pos), which is typically a single letter. If it does, it may\n",
    "    have a disambiguation, a string that distinguishes it from other concepts\n",
    "    with the same text.\n",
    "    This function should be called as follows, where arguments after `text`\n",
    "    are optional:\n",
    "        concept_uri(lang, text, pos, disambiguation...)\n",
    "    `text` and `disambiguation` should be strings that have already been run\n",
    "    through `standardize_text`.\n",
    "    This is a low-level interface. See `standardized_concept_uri` in nodes.py for\n",
    "    a more generally applicable function that also deals with special\n",
    "    per-language handling.\n",
    "    >>> concept_uri('en', 'cat')\n",
    "    '/c/en/cat'\n",
    "    >>> concept_uri('en', 'cat', 'n')\n",
    "    '/c/en/cat/n'\n",
    "    >>> concept_uri('en', 'cat', 'n', 'feline')\n",
    "    '/c/en/cat/n/feline'\n",
    "    >>> concept_uri('en', 'this is wrong')\n",
    "    Traceback (most recent call last):\n",
    "        ...\n",
    "    AssertionError: 'this is wrong' is not in normalized form\n",
    "    \"\"\"\n",
    "    assert ' ' not in text, \"%r is not in normalized form\" % text\n",
    "    if len(more) > 0:\n",
    "        if len(more[0]) != 1:\n",
    "            # We misparsed a part of speech; everything after the text is\n",
    "            # probably junk\n",
    "            more = []\n",
    "        for dis1 in more[1:]:\n",
    "            assert ' ' not in dis1,\\\n",
    "                \"%r is not in normalized form\" % dis1\n",
    "\n",
    "    return join_uri('/c', lang, text, *more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/commonsense/conceptnet5/master/conceptnet5/nodes.py\n",
    "\n",
    "needed things from this file to satisfy cascading dependency:  \n",
    "#from conceptnet5.nodes import standardized_concept_uri, uri_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module constructs URIs for nodes (concepts) in various languages. This\n",
    "puts the tools in conceptnet5.uri together with functions that normalize\n",
    "terms and languages into a standard form.\n",
    "\"\"\"\n",
    "\n",
    "from wordfreq import simple_tokenize\n",
    "\n",
    "def standardize_text(text, token_filter=None):\n",
    "    \"\"\"\n",
    "    Get a string made from the tokens in the text, joined by\n",
    "    underscores. The tokens may have a language-specific `token_filter`\n",
    "    applied to them. See `standardize_as_list()`.\n",
    "\n",
    "        >>> standardize_text(' cat')\n",
    "        'cat'\n",
    "\n",
    "        >>> standardize_text('a big dog', token_filter=english_filter)\n",
    "        'big_dog'\n",
    "\n",
    "        >>> standardize_text('Italian supercat')\n",
    "        'italian_supercat'\n",
    "\n",
    "        >>> standardize_text('a big dog')\n",
    "        'a_big_dog'\n",
    "\n",
    "        >>> standardize_text('a big dog', token_filter=english_filter)\n",
    "        'big_dog'\n",
    "\n",
    "        >>> standardize_text('to go', token_filter=english_filter)\n",
    "        'go'\n",
    "\n",
    "        >>> standardize_text('Test?!')\n",
    "        'test'\n",
    "\n",
    "        >>> standardize_text('TEST.')\n",
    "        'test'\n",
    "\n",
    "        >>> standardize_text('test/test')\n",
    "        'test_test'\n",
    "\n",
    "        >>> standardize_text('   u\\N{COMBINING DIAERESIS}ber\\\\n')\n",
    "        'über'\n",
    "\n",
    "        >>> standardize_text('embedded' + chr(9) + 'tab')\n",
    "        'embedded_tab'\n",
    "\n",
    "        >>> standardize_text('_')\n",
    "        ''\n",
    "\n",
    "        >>> standardize_text(',')\n",
    "        ''\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text.replace('_', ' '))\n",
    "    if token_filter is not None:\n",
    "        tokens = token_filter(tokens)\n",
    "    return '_'.join(tokens)\n",
    "\n",
    "\n",
    "def standardized_concept_uri(lang, text, *more):\n",
    "    \"\"\"\n",
    "    Make the appropriate URI for a concept in a particular language, including\n",
    "    stemming the text if necessary, normalizing it, and joining it into a\n",
    "    concept URI.\n",
    "\n",
    "    Items in 'more' will not be stemmed, but will go through the other\n",
    "    normalization steps.\n",
    "\n",
    "    >>> standardized_concept_uri('en', 'this is a test')\n",
    "    '/c/en/this_is_test'\n",
    "    >>> standardized_concept_uri('en', 'this is a test', 'n', 'example phrase')\n",
    "    '/c/en/this_is_test/n/example_phrase'\n",
    "    \"\"\"\n",
    "    lang = lang.lower()\n",
    "    if lang in LCODE_ALIASES:\n",
    "        lang = LCODE_ALIASES[lang]\n",
    "    if lang == 'en':\n",
    "        token_filter = english_filter\n",
    "    else:\n",
    "        token_filter = None\n",
    "    norm_text = standardize_text(text, token_filter)\n",
    "    more_text = [standardize_text(item, token_filter) for item in more\n",
    "                 if item is not None]\n",
    "    return concept_uri(lang, norm_text, *more_text)\n",
    "\n",
    "#normalized_concept_uri = standardized_concept_uri\n",
    "#standardize_concept_uri = standardized_concept_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Util functions below depend on all the code above\n",
    "\n",
    "The following block of code (particularly standardized_uri() found in the following file: \n",
    "https://raw.githubusercontent.com/commonsense/conceptnet5/master/conceptnet5/vectors/__init__.py\n",
    "\n",
    "Needed all the code blocks above as cascading dependencies. These 2 util functions are themselves dependencies for the debias.py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#from conceptnet5.nodes import standardized_concept_uri, uri_to_label\n",
    "\n",
    "DOUBLE_DIGIT_RE = re.compile(r'[0-9][0-9]')\n",
    "DIGIT_RE = re.compile(r'[0-9]')\n",
    "CONCEPT_RE = re.compile(r'/c/[a-z]{2,3}/.+')\n",
    "\n",
    "\n",
    "def replace_numbers(s):\n",
    "    \"\"\"\n",
    "    Replace digits with # in any term where a sequence of two digits appears.\n",
    "\n",
    "    This operation is applied to text that passes through word2vec, so we\n",
    "    should match it.\n",
    "    \"\"\"\n",
    "    if DOUBLE_DIGIT_RE.search(s):\n",
    "        return DIGIT_RE.sub('#', s)\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "\n",
    "def standardized_uri(language, term):\n",
    "    \"\"\"\n",
    "    Get a URI that is suitable to label a row of a vector space, by making sure\n",
    "    that both ConceptNet's and word2vec's normalizations are applied to it.\n",
    "\n",
    "    If the term already looks like a ConceptNet URI, it will only have its\n",
    "    sequences of digits replaced by #. Otherwise, it will be turned into a\n",
    "    ConceptNet URI in the given language, and then have its sequences of digits\n",
    "    replaced.\n",
    "    \"\"\"\n",
    "    if not CONCEPT_RE.match(term):\n",
    "        term = standardized_concept_uri(language, term)\n",
    "    return replace_numbers(term)\n",
    "\n",
    "\n",
    "def get_vector(frame, label, language=None):\n",
    "    \"\"\"\n",
    "    Returns the row of a vector-space DataFrame `frame` corresponding\n",
    "    to the text `text`. If `language` is set, this can take in plain text\n",
    "    and normalize it to ConceptNet form. Either way, it can also take in\n",
    "    a label that is already in ConceptNet form.\n",
    "    \"\"\"\n",
    "    if frame.index[1].startswith('/c/'):  # This frame has URIs in its index\n",
    "        if not label.startswith('/'):\n",
    "            label = standardized_uri(language, label)\n",
    "        try:\n",
    "            return frame.loc[label]\n",
    "        except KeyError:\n",
    "            return pd.Series(index=frame.columns)\n",
    "    else:\n",
    "        if label.startswith('/'):\n",
    "            label = uri_to_label(label)\n",
    "        try:\n",
    "            return frame.loc[replace_numbers(label)]\n",
    "        except KeyError:\n",
    "            # Return a vector of all NaNs\n",
    "            return pd.Series(index=frame.columns)\n",
    "\n",
    "\n",
    "def normalize_vec(vec):\n",
    "    \"\"\"\n",
    "    L2-normalize a single vector, as a 1-D ndarray or a Series.\n",
    "    \"\"\"\n",
    "    if isinstance(vec, pd.Series):\n",
    "        return normalize(vec.fillna(0).values.reshape(1, -1))[0]\n",
    "    elif isinstance(vec, np.ndarray):\n",
    "        return normalize(vec.reshape(1, -1))[0]\n",
    "    else:\n",
    "        raise TypeError(vec)\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Get the cosine similarity between two vectors -- the cosine of the angle\n",
    "    between them, ranging from -1 for anti-parallel vectors to 1 for parallel\n",
    "    vectors.\n",
    "    \"\"\"\n",
    "    return normalize_vec(vec1).dot(normalize_vec(vec2))\n",
    "\n",
    "\n",
    "def similar_to_vec(frame, vec, limit=50):\n",
    "    # TODO: document the assumptions here\n",
    "    # - frame and vec should be normalized\n",
    "    # - frame should not be made of 8-bit ints\n",
    "    if vec.dot(vec) == 0.:\n",
    "        return pd.Series(data=[], index=[], dtype='f')\n",
    "    similarity = frame.dot(vec)\n",
    "    return similarity.dropna().nlargest(limit)\n",
    "\n",
    "\n",
    "def weighted_average(frame, weight_series):\n",
    "    if isinstance(weight_series, list):\n",
    "        weight_dict = dict(weight_series)\n",
    "        weight_series = pd.Series(weight_dict)\n",
    "    vec = np.zeros(frame.shape[1], dtype='f')\n",
    "\n",
    "    for i, label in enumerate(weight_series.index):\n",
    "        if label in frame.index:\n",
    "            val = weight_series[i]\n",
    "            vec += val * frame.loc[label].values\n",
    "\n",
    "    return pd.Series(data=vec, index=frame.columns, dtype='f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual debias code starts here\n",
    "\n",
    "The lengthy GLOBAL list variables are imported from external file for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# A list of English words referring to nationalities, nations, ethnicities, and\n",
    "# religions. Our goal is to prevent ConceptNet from learning insults and\n",
    "# stereotypes about these classes of people.\n",
    "\n",
    "from conceptnet_code import PEOPLE_BY_ETHNICITY, PEOPLE_BY_BELIEF\n",
    "\n",
    "\n",
    "# A list of things we don't want our semantic space to learn about various\n",
    "# cultures of people. This list doesn't have to be exhaustive; we're modifying\n",
    "# the whole vector space, so nearby terms will also be affected.\n",
    "CULTURE_PREJUDICES = [\n",
    "    'illegal', 'terrorist', 'evil', 'threat',\n",
    "    'dumbass', 'shithead', 'wanker', 'dickhead',\n",
    "    'illiterate', 'ignorant', 'inferior',\n",
    "    'good',\n",
    "    'sexy', 'suave',\n",
    "    'wealthy', 'poor',\n",
    "    'racist', 'slavery',\n",
    "    'torture', 'fascist', 'persecute',\n",
    "    'fraudster', 'rapist', 'robber', 'dodgy', 'perpetrator',\n",
    "]\n",
    "\n",
    "# Numberbatch acquires a \"porn bias\" from the Common Crawl via GloVe.\n",
    "# Because so much of the Web is about porn, words such as 'teen', 'girl', and\n",
    "# 'girlfriend' acquire word associations from porn.\n",
    "#\n",
    "# We handle this and related problems by making an axis of words that refer to\n",
    "# gender or sexual orientation, and exclude them from making associations with\n",
    "# porn and sexually-degrading words.\n",
    "\n",
    "FEMALE_WORDS = [\n",
    "    'woman', 'feminine', 'female',\n",
    "    'girl', 'girlfriend', 'wife', 'mother', 'sister', 'daughter',\n",
    "]\n",
    "\n",
    "MALE_WORDS = [\n",
    "    'man', 'masculine', 'male',\n",
    "    'boy', 'boyfriend', 'husband', 'father', 'brother', 'son'\n",
    "]\n",
    "\n",
    "ORIENTATION_WORDS = [\n",
    "    'gay', 'lesbian', 'bisexual', 'trans', 'transgender'\n",
    "]\n",
    "\n",
    "AGE_WORDS = [\n",
    "    'young', 'teen', 'old'\n",
    "]\n",
    "\n",
    "# FIXME - load from file\n",
    "SEX_PREJUDICES = [\n",
    "    'slut', 'whore', 'shrew', 'bitch', 'faggot',\n",
    "    'sexy', 'fuck', 'fucked', 'fucker', 'nude', 'porn',\n",
    "    'cocksucker'\n",
    "]\n",
    "\n",
    "GENDERED_WORDS = FEMALE_WORDS + MALE_WORDS\n",
    "\n",
    "# Words identified as gender stereotypes by 10 Turkers, in Bolukbasi et al.,\n",
    "# \"Quantifying and Reducing Stereotypes in Word Embeddings\".\n",
    "# https://arxiv.org/pdf/1606.06121.pdf\n",
    "GENDER_NEUTRAL_WORDS = [\n",
    "    'surgeon', 'nurse', 'doctor', 'midwife',\n",
    "    'paramedic', 'registered nurse', 'hummer', 'minivan',\n",
    "    'karate', 'gymnastics', 'woodworking', 'quilting',\n",
    "    'alcoholism', 'eating disorder', 'athlete', 'gymnast',\n",
    "    'neurologist', 'therapist', 'hockey', 'figure skating',\n",
    "    'architect', 'interior designer', 'chauffeur', 'nanny',\n",
    "    'curator', 'librarian', 'pilot', 'flight attendant',\n",
    "    'drug trafficking', 'prostitution', 'musician', 'dancer',\n",
    "    'beer', 'cocktail', 'weightlifting', 'gymnastics',\n",
    "    'headmaster', 'guidance counselor', 'workout', 'pilates',\n",
    "    'home depot', 'jcpenney', 'carpentry', 'sewing',\n",
    "    'accountant', 'paralegal', 'addiction', 'eating disorder',\n",
    "    'professor emeritus', 'associate professor', 'programmer',\n",
    "    'homemaker',  # now augment the conceptnet list with more words from the WEAT paper\n",
    "    'science', 'technology', 'physics', 'chemistry',\n",
    "    'experiment', 'astronomy', 'engineering', 'engineer',\n",
    "    'poetry', 'art', 'dance', 'dancer', 'literature', 'writer',\n",
    "    'novel', 'symphony', 'drama', 'sculpture', 'sculptor',\n",
    "    'math', 'algebra', 'geometry', 'calculus', 'equations',\n",
    "    'computation', 'numbers', 'addition',\n",
    "]\n",
    "\n",
    "\n",
    "def get_weighted_vector(frame, weighted_terms):\n",
    "    \"\"\"\n",
    "    Given a list of (term, weight) pairs, get a unit vector corresponding\n",
    "    to the weighted average of those term vectors.\n",
    "\n",
    "    A simplified version of VectorSpaceWrapper.get_vector().\n",
    "    \"\"\"\n",
    "    total = frame.iloc[0] * 0.\n",
    "    n_terms_used = 0  # debug\n",
    "    for term, weight in weighted_terms:\n",
    "        if term in frame.index:\n",
    "            n_terms_used += 1\n",
    "            vec = frame.loc[term]\n",
    "            total += vec * weight\n",
    "            \n",
    "    # debug\n",
    "    print('got weighted vector for', n_terms_used, 'terms')\n",
    "            \n",
    "    return normalize_vec(total)\n",
    "\n",
    "\n",
    "def get_category_axis(frame, category_examples):\n",
    "    \"\"\"\n",
    "    Get a vector representing the average of several example terms, where\n",
    "    the terms are specified as plain English text.\n",
    "    \"\"\"\n",
    "    # FIXME - this is not friendly to embedding frame's that are not\n",
    "    # indexed in the conceptnet style ('/c/en/example')\n",
    "    return get_weighted_vector(\n",
    "        frame,\n",
    "        [(term, 1.) for term in category_examples]\n",
    "    )\n",
    "\n",
    "\n",
    "def reject_subspace(frame, vecs):\n",
    "    \"\"\"\n",
    "    Return a modification of the vector space `frame` where none of\n",
    "    its rows have any correlation with any rows of `vecs`, by subtracting\n",
    "    the outer product of `frame` with each normalized row of `vecs`.\n",
    "    \"\"\"\n",
    "    current_array = frame.copy().values\n",
    "    for vec in vecs:\n",
    "        if not np.isnan(vec).any():\n",
    "            vec = normalize_vec(vec)\n",
    "            projection = current_array.dot(vec)\n",
    "            np.subtract(current_array, np.outer(projection, vec), out=current_array)\n",
    "\n",
    "    normalize(current_array, norm='l2', copy=False)\n",
    "\n",
    "    current_array = pd.DataFrame(current_array, index=frame.index)\n",
    "    current_array.fillna(0, inplace=True)\n",
    "    return current_array\n",
    "\n",
    "\n",
    "def get_vocabulary_vectors(frame, vocab):\n",
    "    \"\"\"\n",
    "    Given a vocabulary (as a list of English terms), get a sub-frame of the\n",
    "    given DataFrame containing just the known vectors for that vocabulary.\n",
    "    \"\"\"\n",
    "    # FIXME - this is not friendly to embedding frame's that are not\n",
    "    # indexed in the conceptnet style ('/c/en/example')\n",
    "    #uris = [standardized_uri('en', term) for term in vocab]\n",
    "    #return frame.loc[uris].dropna()\n",
    "    return frame.loc[vocab].dropna()\n",
    "\n",
    "\n",
    "def two_class_svm(frame, pos_vocab, neg_vocab):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of word vectors, and lists of words that should be\n",
    "    positive or negative examples of a given category, get a linear\n",
    "    decision boundary between them (and a function that estimates the\n",
    "    probability of the membership of a word in that category) using an SVM.\n",
    "    \"\"\"\n",
    "    pos_vecs = get_vocabulary_vectors(frame, pos_vocab)\n",
    "    pos_values = np.ones(pos_vecs.shape[0])\n",
    "    neg_vecs = get_vocabulary_vectors(frame, neg_vocab)\n",
    "    neg_values = -np.ones(neg_vecs.shape[0])\n",
    "    vecs = np.vstack([pos_vecs.values, neg_vecs.values])\n",
    "    values = np.concatenate([pos_values, neg_values])\n",
    "\n",
    "    svc = svm.SVC(\n",
    "        verbose=False, random_state=0, max_iter=10000, class_weight='balanced',\n",
    "        probability=True, kernel='linear'\n",
    "    )\n",
    "    svc.fit(vecs, values)\n",
    "    return svc\n",
    "\n",
    "def infer_index_format(frame):\n",
    "    \"\"\"\n",
    "    Given a pd.DataFrame of word vectors, determine if the index is in \n",
    "    Conceptnet format (e.g. '/c/en/example_word/pos') or just the words \n",
    "    themselves, as in GloVe or word2vec.\n",
    "    \n",
    "    Decision is made by taking a random sample of rows from the frame and\n",
    "    testing is they start with /c/*\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_size = 1000\n",
    "    rand_sample = list(frame.sample(sample_size, random_state=42).index)\n",
    "    cn_count = 0\n",
    "    \n",
    "    for ind_word in rand_sample:\n",
    "        if CONCEPT_RE.match(ind_word):\n",
    "            cn_count += 1\n",
    "            \n",
    "    if cn_count < 1:\n",
    "        return 'plain'\n",
    "    \n",
    "    cn_fraction = cn_count / sample_size\n",
    "    \n",
    "    if cn_fraction > 0.98:\n",
    "        return 'conceptnet'\n",
    "    else:\n",
    "        return 'mixed'\n",
    "    \n",
    "    \n",
    "def confirm_input_format(ind_type, input_words):\n",
    "    \"\"\"\n",
    "    Attempts to coerce format of example words into a format that matches the\n",
    "    index of the DataFrame of word vectors.\n",
    "    \"\"\"\n",
    "    if ind_type == 'conceptnet':\n",
    "        input_words = [standardized_uri('en', term) for term in input_words]\n",
    "    elif ind_type == 'plain':\n",
    "        for term in input_words:\n",
    "            assert not CONCEPT_RE.match(term), \"%r should not be in conceptnet format\" % term\n",
    "    else:\n",
    "        # input DataFrame of word vectors has index with mixed format\n",
    "        # choose either conceptnet or plain text\n",
    "        raise TypeError(ind_type)\n",
    "    \n",
    "    return input_words\n",
    "    \n",
    "def de_bias_binary(frame, pos_examples, neg_examples, left_examples, right_examples):\n",
    "    \"\"\"\n",
    "    De-bias a distinction that is presumed - for the purposes of de-biasing -\n",
    "    to form two ends of a scale. The prototypical example is male vs. female,\n",
    "    where words that are not inherently defined by gender end up being \"more\n",
    "    male\" or \"more female\" due to stereotypes and biases in the data.\n",
    "\n",
    "    The goal is not to remove the distinction from every word in the system's\n",
    "    vocabulary, only those where making the distinction is inappropriate. A\n",
    "    gender distinction between \"she\" and \"he\" is appropriate. A gender\n",
    "    distinction between \"doctor\" and \"nurse\" is inappropriate.\n",
    "\n",
    "    This function takes in four lists of vocabulary:\n",
    "\n",
    "    - \"Positive examples\": examples of words that *should* be de-biased,\n",
    "      such as \"doctor\" and \"nurse\" in the case of gender.\n",
    "\n",
    "    - \"Negative examples\": examples of words that *should not* be de-biased,\n",
    "      such as \"she\" and \"he\".\n",
    "\n",
    "    - \"Left examples\": words that define one end of the distinction to be\n",
    "      de-biased, such as \"man\".\n",
    "\n",
    "    - \"Right examples\": words that define the other end of the distinction,\n",
    "      such as \"woman\".\n",
    "\n",
    "    The left and right examples are probably also good negative examples:\n",
    "    they appropriately represent the distinction to be made, so they should\n",
    "    not be de-biased.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess the input words here (either convert them to conceptnet \n",
    "    # format or not depending on the format of the DataFrame index)\n",
    "    ind_type = infer_index_format(frame)\n",
    "    pos_examples   = confirm_input_format(ind_type, pos_examples)\n",
    "    neg_examples   = confirm_input_format(ind_type, neg_examples)\n",
    "    left_examples  = confirm_input_format(ind_type, left_examples)\n",
    "    right_examples = confirm_input_format(ind_type, right_examples)\n",
    "    \n",
    "    # Make the SVM that distinguishes positive examples (words that should\n",
    "    # be de-biased) from negative examples.\n",
    "    category_predictor = two_class_svm(frame, pos_examples, neg_examples)\n",
    "\n",
    "    # The SVM can predict the probability, for each vector in the frame, that\n",
    "    # it's in each class. The positive class is column 1 of this prediction.\n",
    "    # This gives us a vector of how much each word in the vocabulary should be\n",
    "    # de-biased.\n",
    "    applicability = category_predictor.predict_proba(frame)[:, 1]\n",
    "\n",
    "    # The bias axis is the vector difference between the average right example\n",
    "    # and the average left example.\n",
    "    print('getting cat axis for right_examples')\n",
    "    r_axis = get_category_axis(frame, right_examples)\n",
    "    print('getting cat axis for left_examples')\n",
    "    l_axis = get_category_axis(frame, left_examples)\n",
    "    bias_axis = r_axis - l_axis\n",
    "\n",
    "    # Make a modified version of the space that projects the bias axis to 0.\n",
    "    # Then weight each row of that space by \"applicability\", the probability\n",
    "    # that each row should be de-biased.\n",
    "    modified_component = reject_subspace(frame, [bias_axis]).mul(applicability, axis=0)\n",
    "\n",
    "    # Make another component representing the vectors that should not be\n",
    "    # de-biased: the original space times (1 - applicability).\n",
    "    result = frame.mul(1 - applicability, axis=0)\n",
    "\n",
    "    # The sum of these two components is the de-biased space, where de-biasing\n",
    "    # applies to each row proportional to its applicability.\n",
    "    np.add(result.values, modified_component.values, out=result.values)\n",
    "    del modified_component\n",
    "\n",
    "    # L_2-normalize the resulting rows in-place.\n",
    "    normalize(result.values, norm='l2', copy=False)\n",
    "    return result, applicability\n",
    "\n",
    "\n",
    "def de_bias_category(frame, category_examples, bias_examples):\n",
    "    \"\"\"\n",
    "    Remove correlations between a class of words that should have biases\n",
    "    removed (category_examples) and a set of words reflecting those biases\n",
    "    (bias_examples). For example, the `category_examples` may be ethnicities,\n",
    "    and `bias_examples` may be stereotypes about them.\n",
    "\n",
    "    The check for whether a word should be de-biased works like\n",
    "    `de_bias_binary`, where the category words are positive examples and the\n",
    "    bias words are negative examples (because the words that define the bias\n",
    "    presumably should not be de-biased).\n",
    "\n",
    "    The words that should be de-biased will have their correlations with\n",
    "    each of the bias words removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess the input words here (either convert them to conceptnet \n",
    "    # format or not depending on the format of the DataFrame index)\n",
    "    ind_type = infer_index_format(frame)\n",
    "    category_examples = confirm_input_format(ind_type, category_examples)\n",
    "    bias_examples     = confirm_input_format(ind_type, bias_examples)\n",
    "    \n",
    "    # Make an SVM that distinguishes words that are in the category to be\n",
    "    # de-biased from words that are not.\n",
    "    category_predictor = two_class_svm(frame, category_examples, bias_examples)\n",
    "\n",
    "    # Predict the probability of each word in the vocabulary being in the\n",
    "    # category.\n",
    "    applicability = category_predictor.predict_proba(frame)[:, 1]\n",
    "    del category_predictor\n",
    "\n",
    "    # Make a matrix of vectors representing the correlations to remove.\n",
    "    # FIXME - this is not friendly to embedding frame's that are not\n",
    "    # indexed in the conceptnet style ('/c/en/example')\n",
    "    #vocab = [standardized_uri('en', term) for term in bias_examples]\n",
    "    #components_to_reject = frame.loc[vocab].values\n",
    "    components_to_reject = frame.loc[bias_examples].values\n",
    "\n",
    "    # Make a modified version of the space that projects the bias vectors to 0.\n",
    "    # Then weight each row of that space by \"applicability\", the probability\n",
    "    # that each row should be de-biased.\n",
    "    modified_component = reject_subspace(frame, components_to_reject).mul(applicability, axis=0)\n",
    "    del components_to_reject\n",
    "\n",
    "    # Make another component representing the vectors that should not be\n",
    "    # de-biased: the original space times (1 - applicability).\n",
    "    result = frame.mul(1 - applicability, axis=0)\n",
    "\n",
    "    # The sum of these two components is the de-biased space, where de-biasing\n",
    "    # applies to each row proportional to its applicability.\n",
    "    np.add(result.values, modified_component.values, out=result.values)\n",
    "    del modified_component\n",
    "\n",
    "    # L_2-normalize the resulting rows in-place.\n",
    "    normalize(result.values, norm='l2', copy=False)\n",
    "    return result, applicability\n",
    "\n",
    "\n",
    "def de_bias_frame(frame):\n",
    "    \"\"\"\n",
    "    Take in a DataFrame representing a semantic space, and make a strong\n",
    "    effort to modify it to remove biases and prejudices against certain\n",
    "    classes of people.\n",
    "\n",
    "    The resulting space attempts not to learn stereotyped associations with\n",
    "    anyone's race, color, religion, national origin, sex, gender presentation,\n",
    "    or sexual orientation.\n",
    "    \"\"\"\n",
    "    newframe, app_ethn = de_bias_category(frame, PEOPLE_BY_ETHNICITY, CULTURE_PREJUDICES + SEX_PREJUDICES)\n",
    "    newframe, app_cult = de_bias_category(newframe, PEOPLE_BY_BELIEF, CULTURE_PREJUDICES + SEX_PREJUDICES)\n",
    "    newframe, app_misc = de_bias_category(newframe, FEMALE_WORDS + MALE_WORDS + ORIENTATION_WORDS + AGE_WORDS, CULTURE_PREJUDICES + SEX_PREJUDICES)\n",
    "    newframe, app_gend = de_bias_binary(newframe, GENDER_NEUTRAL_WORDS, GENDERED_WORDS, MALE_WORDS, FEMALE_WORDS)\n",
    "    return newframe, app_ethn, app_cult, app_misc, app_gend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Load some data to pass to de_bias_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_hdf(filename):\n",
    "    \"\"\"\n",
    "    Load a semantic vector space from an HDF5 file.\n",
    "    HDF5 is a complex format that can contain many instances of different kinds\n",
    "    of data. The convention we use is that the file contains one labeled\n",
    "    matrix, named \"mat\".\n",
    "    \"\"\"\n",
    "    return pd.read_hdf(filename, 'mat', encoding='utf-8', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumbers = load_hdf('data/17.06.mini.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 362891 entries, /c/de/###er to /c/zh/龟\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 833.4+ MB\n"
     ]
    }
   ],
   "source": [
    "cumbers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 362891 entries, /c/de/###er to /c/zh/龟\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 833.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# debiasing only works on float64 i think (what is a better way to change dtypes?\n",
    "# I couldn't seem to get pandas to load these from hdf5 into anything but int8 )\n",
    "for col in cumbers.columns:\n",
    "    cumbers[col] = cumbers[col].astype(np.float64)\n",
    "cumbers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 362891 entries, /c/de/###er to /c/zh/龟\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 843.4+ MB\n",
      "CPU times: user 2min 53s, sys: 1min 3s, total: 3min 57s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deb = de_bias_frame(cumbers)\n",
    "deb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/c/fr/perçoivent',\n",
       " '/c/fr/perçu',\n",
       " '/c/fr/perçue',\n",
       " '/c/fr/perçues',\n",
       " '/c/fr/perçus',\n",
       " '/c/fr/pesant',\n",
       " '/c/fr/pesante',\n",
       " '/c/fr/pesanteur',\n",
       " '/c/fr/peser',\n",
       " '/c/fr/pessimisme']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does the index look like\n",
    "ind = list(cumbers.index)\n",
    "ind[200000:][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def file_stream(fpath, inner_path=None, encoding='utf-8'):\n",
    "    \"\"\"Generator for reading file objects. Handles gzipped and zip\n",
    "    archives.\"\"\"\n",
    "    \n",
    "    if fpath.endswith('.zip'):\n",
    "        with zipfile.ZipFile(fpath) as zfile:\n",
    "            try:\n",
    "                with zfile.open(inner_path) as readfile:\n",
    "                    for line in io.TextIOWrapper(readfile, 'utf-8'):\n",
    "                        yield line\n",
    "            except Exception as e:\n",
    "                print('need a valid inner_path argument, one of these: ', \n",
    "                      zfile.namelist())\n",
    "                raise e  # fixme raise proper excecption\n",
    "                \n",
    "    elif fpath.endswith('.gz'):\n",
    "        raise 'not implemented yet'\n",
    "    else:\n",
    "        with open(fpath, encoding='utf-8') as infile:\n",
    "            for line in (infile):\n",
    "                yield line\n",
    "\n",
    "def load_zipped_embeddings(filename, inner_path=None, \n",
    "                           encoding='utf-8', vocab_only=False):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    \n",
    "    if vocab_only:\n",
    "        for line in file_stream(filename, inner_path, encoding):\n",
    "            if not line.startswith(vocab_only):\n",
    "                continue\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    else:\n",
    "        for line in file_stream(filename, inner_path, encoding):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 300)\n",
      "CPU times: user 1min 17s, sys: 2.83 s, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#zip_path = 'data/glove.840B.300d.zip'\n",
    "#zfile = 'glove.840B.300d.txt'\n",
    "zip_path = 'data/glove.6B.zip'\n",
    "zfile = 'glove.6B.300d.txt'\n",
    "embeddings = load_zipped_embeddings(zip_path, inner_path=zfile)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 400000 entries, the to sandberger\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 918.6+ MB\n"
     ]
    }
   ],
   "source": [
    "embeddings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.191910\n",
       "1      0.350140\n",
       "2     -0.460470\n",
       "3     -0.068022\n",
       "4      0.666540\n",
       "5     -0.023657\n",
       "6     -0.046550\n",
       "7     -0.595320\n",
       "8     -0.229570\n",
       "9     -1.144000\n",
       "10     0.507740\n",
       "11    -0.061745\n",
       "12    -0.109020\n",
       "13     0.773590\n",
       "14     0.197490\n",
       "15    -0.685650\n",
       "16     0.100170\n",
       "17     0.020734\n",
       "18     0.284430\n",
       "19     0.585180\n",
       "20    -0.144190\n",
       "21     0.305860\n",
       "22     0.250030\n",
       "23     0.457050\n",
       "24     0.399640\n",
       "25     0.017308\n",
       "26    -0.296780\n",
       "27     0.111690\n",
       "28    -0.212350\n",
       "29     0.872930\n",
       "         ...   \n",
       "270    0.069308\n",
       "271    0.313710\n",
       "272    0.709770\n",
       "273   -0.344320\n",
       "274   -0.119630\n",
       "275    0.640050\n",
       "276   -1.570500\n",
       "277    0.037442\n",
       "278   -0.014749\n",
       "279   -0.258690\n",
       "280   -0.145470\n",
       "281   -0.266500\n",
       "282   -0.073628\n",
       "283   -0.411070\n",
       "284    0.576080\n",
       "285    0.305980\n",
       "286   -0.359500\n",
       "287    0.523640\n",
       "288   -0.240350\n",
       "289    0.078563\n",
       "290   -0.072383\n",
       "291   -0.627950\n",
       "292   -0.795580\n",
       "293   -0.018491\n",
       "294    0.652040\n",
       "295   -0.260530\n",
       "296    0.001454\n",
       "297   -0.172540\n",
       "298    0.331540\n",
       "299   -0.147650\n",
       "Name: art, Length: 300, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_vec_biased = embeddings.loc['art']\n",
    "art_vec_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.279320\n",
       "1     -0.230860\n",
       "2     -0.019087\n",
       "3     -0.141660\n",
       "4     -0.015263\n",
       "5     -0.597030\n",
       "6     -0.118750\n",
       "7     -0.686740\n",
       "8     -0.218320\n",
       "9     -0.818380\n",
       "10    -0.039471\n",
       "11     0.252810\n",
       "12     0.256940\n",
       "13     0.281090\n",
       "14     0.237340\n",
       "15     0.212450\n",
       "16     0.277160\n",
       "17    -0.367030\n",
       "18     0.061541\n",
       "19    -0.237930\n",
       "20    -0.201850\n",
       "21    -0.139690\n",
       "22    -0.828230\n",
       "23     0.270940\n",
       "24    -0.342580\n",
       "25     0.161700\n",
       "26     0.099299\n",
       "27    -0.278200\n",
       "28     0.471990\n",
       "29     0.246180\n",
       "         ...   \n",
       "270   -0.438290\n",
       "271    0.134150\n",
       "272    0.387900\n",
       "273   -0.303600\n",
       "274    0.148190\n",
       "275   -0.057197\n",
       "276   -1.448500\n",
       "277   -0.303900\n",
       "278    0.043171\n",
       "279    0.084293\n",
       "280   -0.077757\n",
       "281    0.773080\n",
       "282   -0.454120\n",
       "283   -0.456860\n",
       "284    0.260800\n",
       "285    0.774490\n",
       "286   -0.004975\n",
       "287    0.100650\n",
       "288   -0.494700\n",
       "289   -0.052624\n",
       "290    0.132570\n",
       "291   -0.288320\n",
       "292   -0.138510\n",
       "293   -0.152430\n",
       "294    0.138840\n",
       "295    0.070689\n",
       "296    0.115670\n",
       "297   -0.150690\n",
       "298   -0.360910\n",
       "299    0.543410\n",
       "Name: teacher, Length: 300, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_vec_biased = embeddings.loc['teacher']\n",
    "teacher_vec_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cat axis for right_examples\n",
      "got weighted vector for 9 terms\n",
      "getting cat axis for left_examples\n",
      "got weighted vector for 9 terms\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 400000 entries, the to sandberger\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 928.6+ MB\n",
      "CPU times: user 3min 47s, sys: 1min 7s, total: 4min 54s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_deb, app_ethn, app_cult, app_misc, app_gend = de_bias_frame(embeddings)\n",
    "glove_deb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did anything actually change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.045964\n",
       "1      0.054183\n",
       "2     -0.080615\n",
       "3     -0.005628\n",
       "4      0.117184\n",
       "5     -0.016679\n",
       "6     -0.003924\n",
       "7     -0.092368\n",
       "8     -0.053686\n",
       "9     -0.106373\n",
       "10     0.066714\n",
       "11    -0.013379\n",
       "12    -0.007837\n",
       "13     0.110635\n",
       "14     0.045192\n",
       "15    -0.093920\n",
       "16     0.012574\n",
       "17     0.001569\n",
       "18     0.063335\n",
       "19     0.079881\n",
       "20    -0.001634\n",
       "21     0.026795\n",
       "22     0.002635\n",
       "23     0.037535\n",
       "24     0.078586\n",
       "25     0.011516\n",
       "26    -0.039343\n",
       "27     0.033783\n",
       "28    -0.035886\n",
       "29     0.135465\n",
       "         ...   \n",
       "270   -0.004987\n",
       "271    0.054422\n",
       "272    0.126407\n",
       "273   -0.053295\n",
       "274   -0.000271\n",
       "275    0.083734\n",
       "276   -0.148871\n",
       "277   -0.010596\n",
       "278   -0.020537\n",
       "279   -0.039663\n",
       "280   -0.033425\n",
       "281   -0.035942\n",
       "282   -0.001187\n",
       "283   -0.068091\n",
       "284    0.095069\n",
       "285    0.037435\n",
       "286   -0.050581\n",
       "287    0.044748\n",
       "288   -0.053254\n",
       "289    0.006982\n",
       "290    0.002386\n",
       "291   -0.091317\n",
       "292   -0.099861\n",
       "293   -0.021328\n",
       "294    0.099760\n",
       "295   -0.028694\n",
       "296   -0.013165\n",
       "297   -0.026832\n",
       "298    0.059102\n",
       "299   -0.013004\n",
       "Name: art, Length: 300, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_vec = glove_deb.loc['art']\n",
    "art_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.019862\n",
       "1     -0.049448\n",
       "2     -0.010979\n",
       "3      0.007938\n",
       "4     -0.031143\n",
       "5     -0.124095\n",
       "6     -0.025106\n",
       "7     -0.122940\n",
       "8     -0.036864\n",
       "9     -0.013698\n",
       "10    -0.055004\n",
       "11     0.087683\n",
       "12     0.079068\n",
       "13     0.021258\n",
       "14     0.034430\n",
       "15     0.070830\n",
       "16     0.055656\n",
       "17    -0.052178\n",
       "18     0.033013\n",
       "19    -0.009732\n",
       "20     0.028755\n",
       "21    -0.041840\n",
       "22    -0.166747\n",
       "23     0.016043\n",
       "24    -0.040497\n",
       "25     0.051217\n",
       "26     0.002986\n",
       "27    -0.040750\n",
       "28     0.062175\n",
       "29     0.076459\n",
       "         ...   \n",
       "270   -0.066859\n",
       "271    0.046716\n",
       "272    0.052156\n",
       "273   -0.036071\n",
       "274    0.016940\n",
       "275   -0.009257\n",
       "276   -0.061543\n",
       "277   -0.108731\n",
       "278   -0.019918\n",
       "279   -0.010785\n",
       "280    0.000110\n",
       "281    0.119338\n",
       "282   -0.062417\n",
       "283   -0.108852\n",
       "284    0.057933\n",
       "285    0.091050\n",
       "286   -0.023540\n",
       "287   -0.016241\n",
       "288   -0.102136\n",
       "289    0.001571\n",
       "290    0.048383\n",
       "291   -0.015060\n",
       "292    0.020030\n",
       "293   -0.044938\n",
       "294    0.026948\n",
       "295    0.009422\n",
       "296    0.028973\n",
       "297   -0.006327\n",
       "298   -0.034961\n",
       "299    0.085105\n",
       "Name: teacher, Length: 300, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_vec = glove_deb.loc['teacher']\n",
    "teacher_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.94771977,  0.94006975,  0.64661825,  0.95850904,  0.41772661,\n",
       "        0.82307801,  0.9917094 ,  0.37108394,  0.00821813,  0.4261017 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_ethn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230921\n"
     ]
    }
   ],
   "source": [
    "# number of vectors significantly changed for ethnicity\n",
    "changed = 0\n",
    "for v in app_ethn > 0.8:\n",
    "    if v == True:\n",
    "        changed += 1\n",
    "print(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29346\n"
     ]
    }
   ],
   "source": [
    "# number of vectors significantly changed for culture\n",
    "changed = 0\n",
    "for v in app_cult > 0.4:\n",
    "    if v == True:\n",
    "        changed += 1\n",
    "print(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5982\n"
     ]
    }
   ],
   "source": [
    "# number of vectors significantly changed for misc groups\n",
    "changed = 0\n",
    "for v in app_misc > 0.4:\n",
    "    if v == True:\n",
    "        changed += 1\n",
    "print(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395542\n"
     ]
    }
   ],
   "source": [
    "# number of vectors significantly changed for gender\n",
    "changed = 0\n",
    "for v in app_gend > 0.9:\n",
    "    if v == True:\n",
    "        changed += 1\n",
    "print(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save debiased embedding to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "#glove_deb.to_pickle('data/glove.6B.300d_debiased.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# still need to add a size header line to this file (#words, #dimensions)\n",
    "glove_deb.to_csv('data/glove.6B.300d_debiased.txt', \n",
    "                 sep=' ', encoding='utf-8', header=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b16838431726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_deb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_deb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                fmt=[\"%s\"] + [\"%.18e\"]*len(glove_deb.columns))\n\u001b[0m",
      "\u001b[0;32m/Users/mike/.ve/conscience/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miscomplex_X\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "# FIXME - need to specify encoding='utf8' somehow\n",
    "out_path = 'data/glove.6B.300d_debiased_w2vec_fmt.txt'\n",
    "with open(out_path, 'w', encoding='utf8') as f:\n",
    "    np.savetxt(f, \n",
    "               glove_deb.reset_index().values, \n",
    "               delimiter=\" \", \n",
    "               header=\"{} {}\".format(len(glove_deb), len(glove_deb.columns)),\n",
    "               comments=\"\",\n",
    "               fmt=[\"%s\"] + [\"%.18e\"]*len(glove_deb.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
